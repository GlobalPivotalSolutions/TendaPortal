<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- REQUIRED! This is a group of parameters that you _have_ to modify to put an identity to 
     your crawler. -->

  <property>
    <name>http.agent.name</name>
    <value>Arch</value>
    <description>HTTP 'User-Agent' request header. MUST NOT be empty - 
     please set this to a single word uniquely related to your organization.

     NOTE: You should also check other related properties:  

	  http.robots.agents
	  http.agent.description
	  http.agent.url
	  http.agent.email
	  http.agent.version

     and set their values appropriately.

    </description>
  </property>

  <property>
    <name>http.agent.description</name>
    <value>Web crawler</value>
    <description>Further description of our bot- this text is used in
      the User-Agent header.  It appears in parenthesis after the agent name.
    </description>
  </property>

  <property>
    <name>http.agent.url</name>
    <value>http://www.atnf.csiro.au/computing/software/arch</value>
    <description>A URL to advertise in the User-Agent header.  This will 
      appear in parenthesis after the agent name. Custom dictates that this
      should be a URL of a page explaining the purpose and behavior of this
      crawler.
    </description>
  </property>

  <property>
    <name>http.agent.email</name>
    <value>info at example dot com</value>
      <description>An email address to advertise in the HTTP 'From' request
        header and User-Agent header. A good practice is to mangle this
        address (e.g. 'info at example dot com') to avoid spamming.
      </description>
  </property>

  <property>
    <name>http.agent.version</name>
    <value>Arch-1.14</value>
      <description>A version string to advertise in the User-Agent 
      header.</description>
  </property>

  <property>
    <name>http.robots.agents</name>
    <value>Archer,*</value>
    <description>The agent strings we'll look for in robots.txt files,
                 comma-separated, in decreasing order of precedence. You should
                 put the value of http.agent.name as the first agent name, and keep the
                 default * at the end of the list. E.g.: BlurflDev,Blurfl,*
    </description>
  </property>

  <property>
    <name>http.agent.host</name>
    <value>myhost.example.com</value>
    <description>Name or IP address of the host on which the Nutch crawler
     would be running. Currently this is used by 'protocol-httpclient'
     plugin.
    </description>
  </property>


  <!-- This is needed only for debugging under Eclipse
  <property>

    <name>plugin.directory</name>
    <value>./src/plugin</value>
    <description>Directories where nutch plugins are located.  Each
                 element may be a relative or absolute path.  If absolute, it is used
                 as is.  If relative, it is searched for on the classpath.
    </description>
  </property>
  -->

  <property>
    <name>db.update.additions.allowed</name>
    <value>true</value>
    <description>If true, updatedb will add newly discovered URLs, if false
                 only already existing URLs in the CrawlDb will be updated and no new
                 URLs will be added.
    </description>
  </property>

  <property>
    <name>db.ignore.internal.links</name>
    <value>false</value>
    <description>If true, when adding new links to a page, links from
                 the same host are ignored.  This is an effective way to limit the
                 size of the link database, keeping only the highest quality
                 links.
    </description>
  </property>

  <property>
    <name>db.ignore.external.links</name>
    <value>true</value>
    <description>If true, outlinks leading from a page to external hosts
                 will be ignored. This is an effective way to limit the crawl to include
                 only initially injected hosts, without creating complex URLFilters.
    </description>
  </property>

  <property>
    <name>fetcher.threads.per.host</name>
    <value>50</value>
    <description>This number is the maximum number of threads that
                 should be allowed to access a host at one time.
    </description>
  </property>

  <property>
    <name>plugin.excludes</name>
    <value></value>
    <description>Regular expression naming plugin directory names to exclude.  
    </description>
  </property>

  <property>
      <name>http.content.limit</name> <value>150000000</value>
  </property>

  <property>
     <name>file.content.limit</name><value>150000000</value>
  </property>

<property> 
     <name>solr.commit.size</name><value>100</value>
  <description>
  Defines the number of documents to send to Solr in a single update batch.
  Decrease when handling very large documents to prevent Nutch from running
  out of memory.
  </description>  
</property> 

  
  <property>
  <name>query.anchor.boost</name>
  <value>1.0</value>
  <description> Used as a boost for anchor field in Lucene query.
  </description>
</property>

  <property>
   <name>indexer.max.tokens</name>
   <value>100000</value>
   <description>
    The maximum number of tokens that will be indexed for a single field
    in a document. This limits the amount of memory required for
    indexing, so that collections with very large files will not crash
    the indexing process by running out of memory.

    Note that this effectively truncates large documents, excluding
    from the index tokens that occur further in the document. If you
    know your source documents are large, be sure to set this value
    high enough to accomodate the expected size. If you set it to
    -1, then the only limit is your memory, but you should anticipate
    an OutOfMemoryError.
   </description>
  </property>

  <property>
    <name>http.timeout</name>
    <value>300000</value>
    <description>The default network timeout, in milliseconds.</description>
  </property>

  <property>
    <name>fetcher.parse</name>
     <value>false</value>
    <description>If true, fetcher will parse content.</description>
  </property>
  
  <property>
   <name>db.parsemeta.to.crawldb</name>
   <value>canonical</value>
   <description>Comma-separated list of parse metadata keys to transfer to the crawldb (NUTCH-779).
    Assuming for instance that the languageidentifier plugin is enabled, setting the value to 'lang' 
    will copy both the key 'lang' and its value to the corresponding entry in the crawldb.
   </description>
  </property>



</configuration>
